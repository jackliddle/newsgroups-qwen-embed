{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# QWEN-3 Multi-Task Embeddings Exploration: Newsgroups Dataset\n\nThis notebook demonstrates:\n1. Loading the 20 Newsgroups dataset (10 categories)\n2. Embedding texts using QWEN-3-Embedding-0.6B with different task instructions\n3. Comparing Default vs Sentiment vs Topic vs Toxicity task embeddings\n4. Dimensionality reduction with UMAP\n5. Interactive visualization with text tooltips"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport pandas as pd\nimport pickle\nfrom pathlib import Path\nfrom sklearn.datasets import fetch_20newsgroups\nimport umap\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nfrom qwen_embedder import QwenEmbedder, QwenModel, EmbeddingConfig"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Newsgroups Dataset\n",
    "\n",
    "We'll load a subset of the 20 Newsgroups dataset for our demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load newsgroups dataset\n# Using a subset of categories for clarity\ncategories = [\n    'alt.atheism',\n    'comp.graphics',\n    'sci.med',\n    'talk.religion.misc',\n    'rec.sport.baseball',\n    'sci.space',\n    'talk.politics.guns',\n    'talk.politics.mideast',\n    'rec.autos',\n    'sci.crypt'\n]\n\nnewsgroups = fetch_20newsgroups(\n    subset='train',\n    categories=categories,\n    remove=('headers', 'footers', 'quotes'),  # Clean text\n    random_state=42\n)\n\nprint(f\"Total documents: {len(newsgroups.data)}\")\nprint(f\"Categories: {newsgroups.target_names}\")"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sampled 721 documents\n",
      "\n",
      "Category distribution:\n",
      "category\n",
      "sci.med               130\n",
      "sci.space             125\n",
      "alt.atheism           119\n",
      "rec.sport.baseball    118\n",
      "comp.graphics         115\n",
      "talk.religion.misc    114\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Sample a subset for demo (500-1000 samples to keep API costs reasonable)\n",
    "n_samples = 800\n",
    "\n",
    "# Stratified sampling to keep balanced representation\n",
    "np.random.seed(42)\n",
    "indices = []\n",
    "samples_per_category = n_samples // len(categories)\n",
    "\n",
    "for cat_idx in range(len(categories)):\n",
    "    cat_indices = np.where(newsgroups.target == cat_idx)[0]\n",
    "    selected = np.random.choice(cat_indices, size=min(samples_per_category, len(cat_indices)), replace=False)\n",
    "    indices.extend(selected)\n",
    "\n",
    "indices = np.array(indices)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Create DataFrame for easier handling\n",
    "df = pd.DataFrame({\n",
    "    'text': [newsgroups.data[i] for i in indices],\n",
    "    'category': [newsgroups.target_names[newsgroups.target[i]] for i in indices],\n",
    "    'category_id': [newsgroups.target[i] for i in indices]\n",
    "})\n",
    "\n",
    "# Clean up texts: remove very short documents\n",
    "df['text_length'] = df['text'].str.len()\n",
    "df = df[df['text_length'] > 100].reset_index(drop=True)\n",
    "\n",
    "# Truncate very long texts to avoid token limits\n",
    "df['text_clean'] = df['text'].str[:2000]\n",
    "\n",
    "print(f\"\\nSampled {len(df)} documents\")\n",
    "print(f\"\\nCategory distribution:\")\n",
    "print(df['category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample document:\n",
      "Category: sci.med\n",
      "Text: \n",
      "Flights of fancy, and other irrational approaches, are common.  The crucial\n",
      "thing is not to sit around just having fantasies; they aren't of any use\n",
      "unless they make you do some experiments.  I've known a lot of scientists\n",
      "whose fantasies lead them on to creative work; usually they won't admit\n",
      "out loud what the fantasy was, prior to the consumption of a few beers.\n",
      "\n",
      "(Simple example: Warren Jelinek noticed an extremely heavy band on a DNA\n",
      "electrophoresis gel of human ALU fragments.  He got very e...\n"
     ]
    }
   ],
   "source": [
    "# Preview a sample document\n",
    "print(\"Sample document:\")\n",
    "print(f\"Category: {df.iloc[0]['category']}\")\n",
    "print(f\"Text: {df.iloc[0]['text_clean'][:500]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Embed Texts with QWEN-3 (Multi-Task Comparison)\n\nWe'll embed the same texts using QWEN-3-Embedding-0.6B with four different task instructions:\n- **Default**: No instruction (general-purpose)\n- **Sentiment**: \"Classify the sentiment of the given text as positive, negative, or neutral\"\n- **Topic**: \"Identify the topic or theme of the given text\"\n- **Toxicity**: \"Classify the given text as either toxic or not toxic\""
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure embedder\n",
    "config = EmbeddingConfig(\n",
    "    model=QwenModel.SMALL,\n",
    "    batch_size=32,\n",
    "    max_concurrent=10\n",
    ")\n",
    "\n",
    "# Cache file for embeddings\n",
    "cache_dir = Path('embeddings')\n",
    "cache_dir.mkdir(exist_ok=True)\n",
    "cache_file = cache_dir / 'newsgroups_embeddings.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Define the tasks we want to compare\ntasks = {\n    'default': None,  # No instruction\n    'sentiment': \"Classify the sentiment of the given text as positive, negative, or neutral\",\n    'topic': \"Identify the topic or theme of the given text\",\n    'toxicity': \"Classify the given text as either toxic or not toxic\"\n}\n\n# Cache file for embeddings\ncache_dir = Path('embeddings')\ncache_dir.mkdir(exist_ok=True)\ncache_file = cache_dir / 'newsgroups_multitask_embeddings.pkl'\n\n# Embed texts for all tasks (or load from cache)\nall_embeddings = {}\n\nif cache_file.exists():\n    print(\"Loading embeddings from cache...\")\n    with open(cache_file, 'rb') as f:\n        cache_data = pickle.load(f)\n        cached_embeddings = cache_data['embeddings']\n        cached_texts = cache_data['texts']\n    \n    # Verify cache matches current data\n    if cached_texts == df['text_clean'].tolist():\n        all_embeddings = cached_embeddings\n        print(f\"Loaded embeddings for {len(all_embeddings)} tasks from cache\")\n        for task_name in all_embeddings.keys():\n            print(f\"  - {task_name}: {len(all_embeddings[task_name])} embeddings\")\n    else:\n        print(\"Cache mismatch, re-embedding...\")\n        cache_file.unlink()\n\nif not all_embeddings:\n    print(f\"Embedding texts with {len(tasks)} different task instructions...\")\n    \n    async def embed_all_tasks():\n        results = {}\n        async with QwenEmbedder(config=config) as embedder:\n            for task_name, instruction in tasks.items():\n                print(f\"\\n{'='*60}\")\n                print(f\"Task: {task_name}\")\n                if instruction:\n                    print(f\"Instruction: {instruction}\")\n                else:\n                    print(f\"Instruction: None (default embeddings)\")\n                print(f\"{'='*60}\")\n                \n                embeddings = await embedder.embed_async(\n                    df['text_clean'].tolist(),\n                    task_instruction=instruction,\n                    show_progress=True\n                )\n                results[task_name] = embeddings\n        return results\n    \n    import asyncio\n    all_embeddings = await embed_all_tasks()\n    \n    # Cache the results\n    print(\"\\nSaving embeddings to cache...\")\n    with open(cache_file, 'wb') as f:\n        pickle.dump({\n            'embeddings': all_embeddings,\n            'texts': df['text_clean'].tolist()\n        }, f)\n    \n    print(f\"Saved embeddings for {len(all_embeddings)} tasks to cache\")\n\n# Convert to numpy arrays\nembeddings_arrays = {\n    task_name: np.array(embeddings) \n    for task_name, embeddings in all_embeddings.items()\n}\n\nprint(f\"\\nEmbeddings shapes:\")\nfor task_name, arr in embeddings_arrays.items():\n    print(f\"  {task_name}: {arr.shape}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Dimensionality Reduction with UMAP\n\nWe'll apply UMAP to each embedding set to reduce to 2D for visualization."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"Applying UMAP to all embedding sets...\")\n\n# Apply UMAP to each task's embeddings\numap_embeddings = {}\n\nfor task_name, embeddings_array in embeddings_arrays.items():\n    print(f\"\\nProcessing {task_name}...\")\n    \n    umap_reducer = umap.UMAP(\n        n_components=2,\n        n_neighbors=15,\n        min_dist=0.1,\n        random_state=42,\n        verbose=False\n    )\n    \n    reduced = umap_reducer.fit_transform(embeddings_array)\n    umap_embeddings[task_name] = reduced\n    print(f\"  {task_name}: {embeddings_array.shape} -> {reduced.shape}\")\n\nprint(\"\\nDimensionality reduction complete!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Add UMAP coordinates for each task to dataframe\nfor task_name, reduced in umap_embeddings.items():\n    df[f'{task_name}_x'] = reduced[:, 0]\n    df[f'{task_name}_y'] = reduced[:, 1]\n\n# Create shortened text for tooltips (first 200 chars)\ndf['text_preview'] = df['text_clean'].str[:200] + '...'\n\nprint(\"Added UMAP coordinates to dataframe:\")\nprint(df[['category', 'default_x', 'default_y', 'sentiment_x', 'sentiment_y', 'topic_x', 'topic_y', 'toxicity_x', 'toxicity_y']].head())"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Multi-Task Visualization Comparison\n\nCreate 4-panel visualization comparing how different task instructions affect the embedding space."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create 4-panel comparison: Default vs Sentiment vs Topic vs Toxicity\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\n# Color palette for categories\ncolor_map = {\n    cat: px.colors.qualitative.Set2[i % len(px.colors.qualitative.Set2)]\n    for i, cat in enumerate(df['category'].unique())\n}\n\n# Task titles for display\ntask_titles = {\n    'default': 'Default (No Instruction)',\n    'sentiment': 'Sentiment Task',\n    'topic': 'Topic Classification Task',\n    'toxicity': 'Toxicity Detection Task'\n}\n\nfig = make_subplots(\n    rows=1, cols=4,\n    subplot_titles=[task_titles[task] for task in ['default', 'sentiment', 'topic', 'toxicity']],\n    horizontal_spacing=0.05\n)\n\n# Create a plot for each task\nfor col_idx, task_name in enumerate(['default', 'sentiment', 'topic', 'toxicity'], start=1):\n    for category in df['category'].unique():\n        mask = df['category'] == category\n        fig.add_trace(\n            go.Scatter(\n                x=df[mask][f'{task_name}_x'],\n                y=df[mask][f'{task_name}_y'],\n                mode='markers',\n                name=category,\n                marker=dict(\n                    size=6,\n                    color=color_map[category],\n                    opacity=0.7,\n                    line=dict(width=0.5, color='white')\n                ),\n                text=df[mask]['text_preview'],\n                hovertemplate='<b>%{fullData.name}</b><br>' +\n                             '<br>%{text}<br>' +\n                             '<extra></extra>',\n                legendgroup=category,\n                showlegend=(col_idx == 1)  # Only show legend for first plot\n            ),\n            row=1, col=col_idx\n        )\n\n# Update layout\nfig.update_layout(\n    title_text='QWEN-3 Multi-Task Embedding Comparison (UMAP Projection)',\n    title_x=0.5,\n    height=600,\n    width=2400,\n    hovermode='closest',\n    legend=dict(\n        yanchor=\"top\",\n        y=0.99,\n        xanchor=\"left\",\n        x=1.01,\n        title=\"Newsgroup Category\"\n    )\n)\n\n# Update axes labels\nfor col_idx in range(1, 5):\n    fig.update_xaxes(title_text=\"UMAP Dimension 1\", row=1, col=col_idx)\n    fig.update_yaxes(title_text=\"UMAP Dimension 2\", row=1, col=col_idx)\n\nfig.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Individual Task Plots for Detailed Exploration"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Default embeddings plot\nfig_default = px.scatter(\n    df,\n    x='default_x',\n    y='default_y',\n    color='category',\n    hover_data={'text_preview': True, 'default_x': False, 'default_y': False, 'category': False},\n    title='Default Embeddings (No Task Instruction) - UMAP Projection',\n    width=900,\n    height=700,\n    color_discrete_map=color_map\n)\n\nfig_default.update_traces(\n    marker=dict(size=8, opacity=0.7, line=dict(width=0.5, color='white')),\n    hovertemplate='<b>%{fullData.name}</b><br><br>%{customdata[0]}<br><extra></extra>'\n)\n\nfig_default.update_layout(hovermode='closest')\nfig_default.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Sentiment task embeddings plot\nfig_sentiment = px.scatter(\n    df,\n    x='sentiment_x',\n    y='sentiment_y',\n    color='category',\n    hover_data={'text_preview': True, 'sentiment_x': False, 'sentiment_y': False, 'category': False},\n    title='Sentiment Task Embeddings - UMAP Projection',\n    width=900,\n    height=700,\n    color_discrete_map=color_map\n)\n\nfig_sentiment.update_traces(\n    marker=dict(size=8, opacity=0.7, line=dict(width=0.5, color='white')),\n    hovertemplate='<b>%{fullData.name}</b><br><br>%{customdata[0]}<br><extra></extra>'\n)\n\nfig_sentiment.update_layout(hovermode='closest')\nfig_sentiment.show()"
  },
  {
   "cell_type": "code",
   "source": "# Topic task embeddings plot\nfig_topic = px.scatter(\n    df,\n    x='topic_x',\n    y='topic_y',\n    color='category',\n    hover_data={'text_preview': True, 'topic_x': False, 'topic_y': False, 'category': False},\n    title='Topic Classification Task Embeddings - UMAP Projection',\n    width=900,\n    height=700,\n    color_discrete_map=color_map\n)\n\nfig_topic.update_traces(\n    marker=dict(size=8, opacity=0.7, line=dict(width=0.5, color='white')),\n    hovertemplate='<b>%{fullData.name}</b><br><br>%{customdata[0]}<br><extra></extra>'\n)\n\nfig_topic.update_layout(hovermode='closest')\nfig_topic.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Toxicity task embeddings plot\nfig_toxicity = px.scatter(\n    df,\n    x='toxicity_x',\n    y='toxicity_y',\n    color='category',\n    hover_data={'text_preview': True, 'toxicity_x': False, 'toxicity_y': False, 'category': False},\n    title='Toxicity Detection Task Embeddings - UMAP Projection',\n    width=900,\n    height=700,\n    color_discrete_map=color_map\n)\n\nfig_toxicity.update_traces(\n    marker=dict(size=8, opacity=0.7, line=dict(width=0.5, color='white')),\n    hovertemplate='<b>%{fullData.name}</b><br><br>%{customdata[0]}<br><extra></extra>'\n)\n\nfig_toxicity.update_layout(hovermode='closest')\nfig_toxicity.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 6. Save Visualizations",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Save interactive HTML files\nviz_dir = Path('visualizations')\nviz_dir.mkdir(exist_ok=True)\n\n# Save comparison and individual plots\nfig.write_html(viz_dir / 'multitask_comparison.html')\nfig_default.write_html(viz_dir / 'default_embeddings.html')\nfig_sentiment.write_html(viz_dir / 'sentiment_embeddings.html')\nfig_topic.write_html(viz_dir / 'topic_embeddings.html')\nfig_toxicity.write_html(viz_dir / 'toxicity_embeddings.html')\n\nprint(\"Saved visualizations to:\", viz_dir.absolute())\nprint(\"\\nFiles created:\")\nprint(\"  - multitask_comparison.html (4-panel comparison)\")\nprint(\"  - default_embeddings.html\")\nprint(\"  - sentiment_embeddings.html\")\nprint(\"  - topic_embeddings.html\")\nprint(\"  - toxicity_embeddings.html\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Analysis Notes\n\n**Hover over points** to see the text content of each document.\n\n### Comparing Task Instructions:\n\nThis notebook explores how task-specific instructions affect QWEN-3 embeddings:\n\n1. **Default Embeddings** - General-purpose, no task optimization\n2. **Sentiment Task** - Optimized for sentiment classification (positive/negative/neutral)\n3. **Topic Task** - Optimized for topic/theme identification\n4. **Toxicity Task** - Optimized for detecting toxic/inflammatory discourse\n\n### What to Look For:\n\n1. **Cluster Quality**: Which task produces the best newsgroup category separation?\n2. **Task Alignment**: Does the topic task create better clusters than default for newsgroup categorization?\n3. **Orthogonal Dimensions**: How does toxicity detection differ from sentiment and topic?\n4. **Cross-Topic Patterns**: Does toxicity appear across multiple newsgroups (e.g., political vs technical)?\n5. **Semantic Organization**: How do different instructions reorganize the embedding space?\n\n### Expected Observations:\n\n- **Topic task** should produce the best category separation (aligned with newsgroup classification)\n- **Sentiment task** may group texts by emotional tone rather than category\n- **Toxicity task** should reveal discourse patterns - political/religious groups may show different toxicity distributions than technical/hobby groups\n- **Default embeddings** provide a balanced, general-purpose representation\n- UMAP preserves both local clusters and global structure across all tasks\n\n### Key Insight:\n\nTask-specific instructions can significantly reshape the embedding space to optimize for particular downstream applications. The \"right\" instruction depends on your use case!\n\n### Categories Analyzed (10 total):\n\n- **Religious/Political**: alt.atheism, talk.religion.misc, talk.politics.guns, talk.politics.mideast\n- **Scientific/Technical**: sci.med, sci.space, sci.crypt, comp.graphics\n- **Recreation**: rec.sport.baseball, rec.autos",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newsgroups-qwen-embed-stTfPPiI-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}